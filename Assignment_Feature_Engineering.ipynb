{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Q1) What is a parameter?\n",
        "**Ans:-** A parameter in Machine Learning (ML) is a configuration variable that is internal to the model and whose value is learned or estimated automatically from the training data.\n",
        "\n",
        "These learned values are what the model ultimately uses to make predictions on new, unseen data, essentially defining the skill of the model on a specific problem.\n",
        "\n",
        "#Q2) What is correlation? What does negative correlation mean?\n",
        "**Ans:-** Correlation is a statistical measure that expresses the extent to which two variables are linearly related—meaning they change together. It quantifies the strength and direction of this relationship.\n",
        "\n",
        "Negative correlation, also known as an inverse correlation, means that the two variables tend to move in opposite directions.\n",
        "When the value of one variable increases, the value of the other variable tends to decrease, and vice-versa.\n",
        "\n",
        "#Q3) Define Machine Learning. What are the main components in Machine Learning?\n",
        "**Ans:-** Machine Learning is a subset of Artificial Intelligence that enables a system to learn from data, identify patterns, and make decisions or predictions with minimal explicit programming.\n",
        "\n",
        "Main Components of Machine Learning\n",
        "\n",
        "\n",
        "1. Data\n",
        "2. Model\n",
        "3. Features\n",
        "4. Algorithm\n",
        "5. Training\n",
        "6. Evaluation\n",
        "7. Prediction/Inference\n",
        "8. Feedback (optional)\n",
        "\n",
        "\n",
        "#Q4) How does loss value help in determining whether the model is good or not?\n",
        "**Ans:-** The loss value is the primary numerical measure of how good a machine learning model is because it quantifies the difference between the model's predictions and the actual values.A lower loss value is better, indicating higher accuracy and a better fit to the data. Crucially, by comparing the loss on the training data to the loss on the validation data, you can diagnose if the model is truly good (low loss on both) or suffering from problems like overfitting (low training loss, high validation loss) or underfitting (high loss on both).\n",
        "\n",
        "#Q5) What are continuous and categorical variables?\n",
        "**Ans:-** Continuous Variables-\n",
        "\n",
        "Quantitative variables that can take any value within a given range, including decimals and fractions.They are typically the result of measuring something.\n",
        "\n",
        "example:-\n",
        "\n",
        "\n",
        "*   Height: $175.5\\text{ cm}$, $175.51\\text{ cm}$, $175.512\\text{ cm}$\n",
        "*   Temperature: $25.5^\\circ\\text{C}$\n",
        "*   Time: $10.32$ seconds\n",
        "*   Weight: $75.8\\text{ kg}$\n",
        "\n",
        "Categorical Variables-\n",
        "\n",
        "Qualitative variables that represent distinct categories or groups. They are typically the result of sorting or classifying something.\n",
        "\n",
        "example:-\n",
        "\n",
        "Gender: Male, Female, Non-binary\n",
        "\n",
        "Marital Status: Single, Married, Divorced\n",
        "\n",
        "Blood Type: A, B, AB, O\n",
        "\n",
        "Education Level (Ordinal): High School, Bachelor's, Master's (These have an order, but are still distinct categories).\n",
        "\n",
        "\n",
        "#Q6) How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "**Ans:-** When using categorical variables in machine learning, the essential step is encoding, which converts the non-numerical categories into a format that mathematical models can process. The choice of technique depends on the variable type: for categories with a natural order (ordinal data, like T-shirt size), Ordinal Encoding (assigning ordered integers, e.g., $1, 2, 3$) is used to preserve the rank. For categories without any inherent order (nominal data, like color or gender), One-Hot Encoding is typically preferred; this method creates a new binary (0 or 1) column for each unique category, preventing the model from assuming a false order. Other techniques, like Target Encoding (replacing a category with the mean of the target variable for that group), are used for features with a very large number of unique categories (high cardinality) to manage dimensionality while still capturing predictive information.\n",
        "\n",
        "#Q7) What do you mean by training and testing a dataset?\n",
        "**Ans:-** Training and testing a dataset refers to the process of splitting data into two parts: the training set is used to teach the machine learning model by helping it learn patterns from the data, while the testing set is used to evaluate the model's performance on new, unseen data. This helps check how well the model generalizes and avoids overfitting.\n",
        "\n",
        "#Q8) What is sklearn.preprocessing?\n",
        "**Ans:-** sklearn.preprocessing is a module in Scikit-learn that provides tools to prepare and transform data before feeding it into a machine learning model. It includes functions for scaling, normalizing, encoding categorical variables, and handling missing values, which are essential steps to improve model performance and accuracy.\n",
        "\n",
        "\n",
        "#Q9) What is a Test set?\n",
        "**Ans:-** A test set is a portion of the dataset that is kept separate from the training process and used only to evaluate the final performance of a trained machine learning model. It helps determine how well the model can generalize to new, unseen data, giving a realistic estimate of its accuracy and effectiveness in real-world scenarios.\n",
        "\n",
        "#Q10) How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "\n",
        "**Ans:-** To split data for model fitting in Python, we commonly use train_test_split() from sklearn.model_selection. It divides the dataset into training and testing sets\n",
        "\n",
        "To approach a machine learning problem, we typically follow these steps:\n",
        "\n",
        "1. Understand the problem and data,\n",
        "2. Preprocess the data (cleaning, encoding, scaling),\n",
        "3. Split the data into training and testing sets,\n",
        "4. Select and train a model,\n",
        "5. Evaluate the model using metrics, and\n",
        "6. Tune or improve the model as needed.\n",
        "\n",
        "#Q11) Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "**Ans:-** We perform Exploratory Data Analysis (EDA) before fitting a model to better understand the data, uncover important patterns, detect anomalies or missing values, and identify relationships between variables. EDA helps ensure data quality, guides feature selection or engineering, and informs the choice of algorithms. Without EDA, we risk feeding poor-quality or misleading data into the model, which can lead to inaccurate predictions and ineffective results.\n",
        "\n",
        "#Q12) What is correlation?\n",
        "**Ans:-** correlation is a statistical measure that expresses the extent to which two variables are linearly related—meaning they change together. It quantifies the strength and direction of this relationship.\n",
        "\n",
        "#Q13) What does negative correlation mean?\n",
        "**Ans:-** Negative correlation, also known as an inverse correlation, means that the two variables tend to move in opposite directions.\n",
        "When the value of one variable increases, the value of the other variable tends to decrease, and vice-versa.\n",
        "\n",
        "#Q14) How can you find correlation between variables in Python?\n",
        "**Ans:-** You can find the correlation between variables in Python using the .corr() method from pandas, which calculates the Pearson correlation coefficient by default.\n",
        "\n",
        "#Q15) What is causation? Explain difference between correlation and causation with an example.\n",
        "**Ans:-** Causation means that one event or variable directly causes or brings about a change in another. Unlike correlation, which only shows a relationship or pattern between variables, causation implies a cause-and-effect connection where one factor is responsible for the outcome of the other.\n",
        "\n",
        "Correlation means there is a statistical relationship between two variables—they tend to move together in some pattern (either both increase, both decrease, or one increases while the other decreases). However, correlation does not imply that one variable causes the other to happen.\n",
        "\n",
        "Example:-There is a strong positive correlation between ice cream sales and drowning incidents — both tend to increase during summer. However, ice cream sales do not cause drowning.\n",
        "\n",
        "Causation means that one variable directly affects or causes a change in the other. There is a cause-and-effect relationship.\n",
        "\n",
        "Example:- The rise in temperature during summer causes both more people to buy ice cream and more people to swim, which unfortunately leads to an increase in drowning incidents. Here, temperature is the causal factor.\n",
        "\n",
        "#Q16) What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "**Ans:-** An **optimizer** is an algorithm used in machine learning to adjust a model’s parameters during training to minimize the loss function and improve accuracy. Common types include:\n",
        "\n",
        "* **Gradient Descent (GD):** Updates parameters using the whole dataset’s gradient; simple but slow for large data.\n",
        "* **Stochastic Gradient Descent (SGD):** Updates parameters using one random data point at a time, making training faster but noisier.\n",
        "* **Mini-batch Gradient Descent:** Uses small batches of data for updates, balancing speed and stability.\n",
        "* **Momentum:** Accelerates updates by adding a fraction of the previous step to smooth and speed convergence.\n",
        "* **Adam:** Combines momentum and adaptive learning rates for efficient and robust training, widely used in deep learning.\n",
        "\n",
        "Each optimizer helps the model learn by tweaking parameters differently to reach the best performance.\n",
        "\n",
        "\n",
        "#Q17) What is sklearn.linear_model ?\n",
        "\n",
        "**Ans:-** sklearn.linear_model is a module in scikit-learn (often imported as sklearn), a popular Python library for machine learning. This module provides implementations of various linear models used for regression and classification tasks.\n",
        "\n",
        "#Q18) What does model.fit() do? What arguments must be given?\n",
        "**Ans:-** model.fit() is a core method in scikit-learn used to train or fit a machine learning model on a dataset. Essentially, it:\n",
        "\n",
        "Takes your training data,\n",
        "\n",
        "Learns the relationships or patterns in that data,\n",
        "\n",
        "Adjusts the internal parameters of the model accordingly,\n",
        "\n",
        "So the model can make predictions on new, unseen data.\n",
        "\n",
        "For example, in linear regression, fit() finds the best-fitting line by calculating the coefficients that minimize the error between predicted and actual target values.\n",
        "\n",
        "Arguments to model.fit()\n",
        "\n",
        "Typically, fit() requires at least two arguments:\n",
        "\n",
        "X — Feature matrix (input data)\n",
        "\n",
        "Usually a 2D array-like structure (e.g., numpy array, pandas DataFrame)\n",
        "\n",
        "Shape: (n_samples, n_features) — number of samples × number of features\n",
        "\n",
        "y — Target vector (output data)\n",
        "\n",
        "The labels or values you want to predict\n",
        "\n",
        "Usually 1D array-like for regression or classification\n",
        "\n",
        "Shape: (n_samples,) (or (n_samples, n_targets) for multi-output problems)\n",
        "\n",
        "#Q19) What does model.predict() do? What arguments must be given?\n",
        "**Ans:-** model.predict() is the method you use after fitting the model to make predictions on new data. It takes input features and outputs predicted target values based on what the model learned during fit().\n",
        "\n",
        "The main argument is X — the feature matrix (input data) for which you want predictions.\n",
        "\n",
        "This should be in the same format and shape as the data used for training.\n",
        "\n",
        "Shape: (n_samples, n_features)\n",
        "\n",
        "#Q20) What are continuous and categorical variables?\n",
        "**Ans:-** Continuous Variables-\n",
        "\n",
        "Quantitative variables that can take any value within a given range, including decimals and fractions.They are typically the result of measuring something.\n",
        "\n",
        "example:-\n",
        "\n",
        "\n",
        "*   Height: $175.5\\text{ cm}$, $175.51\\text{ cm}$, $175.512\\text{ cm}$\n",
        "*   Temperature: $25.5^\\circ\\text{C}$\n",
        "*   Time: $10.32$ seconds\n",
        "*   Weight: $75.8\\text{ kg}$\n",
        "\n",
        "Categorical Variables-\n",
        "\n",
        "Qualitative variables that represent distinct categories or groups. They are typically the result of sorting or classifying something.\n",
        "\n",
        "example:-\n",
        "\n",
        "Gender: Male, Female, Non-binary\n",
        "\n",
        "Marital Status: Single, Married, Divorced\n",
        "\n",
        "Blood Type: A, B, AB, O\n",
        "\n",
        "Education Level (Ordinal): High School, Bachelor's, Master's (These have an order, but are still distinct categories).\n",
        "\n",
        "#Q21) What is feature scaling? How does it help in Machine Learning?\n",
        "**Ans:-** Feature scaling is the process of standardizing or normalizing the range of independent variables (features) in our data.\n",
        "\n",
        "Feature scaling help in Machine Learning by :-\n",
        "\n",
        "Improves convergence speed: Algorithms like gradient descent converge faster if features are on similar scales.\n",
        "\n",
        "Prevents one feature from dominating: If one feature has a huge range, it can overshadow others in distance-based algorithms (like K-Nearest Neighbors, SVM, or K-Means).\n",
        "\n",
        "Improves model performance: Many models (like Logistic Regression, Neural Networks, SVMs) perform better or more reliably after scaling.\n",
        "\n",
        "Necessary for distance-based algorithms: Algorithms that use distances (Euclidean, Manhattan) assume all features contribute equally; without scaling, this is not true.\n",
        "\n",
        "#Q22) How do we perform scaling in Python?\n",
        "**Ans:-** Performing feature scaling in Python is super straightforward, especially using scikit-learn (sklearn), which provides ready-made tools for common scaling methods.\n",
        "\n",
        "Import scaler from sklearn.preprocessing\n",
        "\n",
        "Use .fit_transform() on training data to learn scaling parameters and scale data\n",
        "\n",
        "Use .transform() on new/unseen data to scale with the same parameters\n",
        "\n",
        "#Q23) What is sklearn.preprocessing?\n",
        "**Ans:-** sklearn.preprocessing is a module in scikit-learn that provides a collection of utility functions and classes for preprocessing our data before feeding it into a machine learning model.\n",
        "\n",
        "#Q24) How do we split data for model fitting (training and testing) in Python?\n",
        "**Ans:-** In Python, we typically split data into training and testing sets using train_test_split from sklearn.model_selection. This function randomly divides the dataset, so the model can be trained on one part and tested on another to evaluate its performance.\n",
        "\n",
        "#Q25) Explain data encoding?\n",
        "**Ans:-** Data encoding is the process of converting categorical (non-numeric) data into a numerical format so that machine learning algorithms can process it — since most models work only with numbers.\n",
        "\n",
        "Machine learning models can’t interpret text or labels as categories unless they’re numerically represented. Encoding ensures that the model understands the categories without assigning unintended meaning or order."
      ],
      "metadata": {
        "id": "BvrSppx9Wt9I"
      }
    }
  ]
}